---
version: "v1"
appName: "myappname"
env: "dev"

host: "aws.tritonhq.io"

image: "nginx"
imageTag: "latest"
imageDigest: ""
imagePullPolicy: "IfNotPresent" # If imageTag = "latest", set this to "Always"
imagePullSecrets:
  - name: image-pull-secret

replicas: 2
resources:
  requests:
    cpu: "100m"
    memory: "200M"
  # limits:
  #   cpu: "200m"
  #   memory: "256M"

persistence:
  enabled: false
  labels: {}
  annotations: {}
  volumeMounts:
    - name: "appVolumeMount"
      mountPath: "/usr/share/myapp/data"

volumeClaimTemplate:
  accessModes: ["ReadWriteOnce"]
  storageClassName: "standard"
  resources:
    requests:
      storage: "600M"

services:
  - name: db3333 # appname will be prefixed, needs to be unique
    protocol: tls # http, tls, tcp
    port: 3333 # number only
    ingress:
      enabled: true
      tls: "terminated" # "terminated" or "" (not applicable) or "passthrough" (to be implemented)
      port: 443
      path: "/device-backend/" # prefix regex match
      # httpsRedirect: false # TBD implementation - can't be set at service level but will need to be cluster level
      # hosts: # OR matched
      #   - "*"
      #   - "aws.tritonhq.io"
      # headers: unused # To be implemented
  - name: hs8080 # appname will be prefixed, needs to be unique
    protocol: tls # http, tls, tcp
    port: 8080 # number only
    ingress:
      enabled: true
      tls: "terminated" # "terminated" or "" (not applicable) or "passthrough" (to be implemented)
      port: 443
      path: "/hasura/" # prefix regex match
  - name: mq8883 # appname will be prefixed, needs to be unique
    protocol: tcp # http, tls, tcp
    port: 8883 # number only
    ingress:
      enabled: true
      tls: "" # "terminated" or "" (not applicable) or "passthrough" (to be implemented)
      port: 8883
      path: "/" # prefix regex match
  - name: mq1883 # appname will be prefixed, needs to be unique
    protocol: tcp # http, tls, tcp
    port: 1883 # number only
    ingress:
      enabled: true
      tls: "" # "terminated" or "" (not applicable) or "passthrough" (to be implemented)
      port: 1883
      path: "/" # prefix regex match
  - name: wp80 # appname will be prefixed, needs to be unique
    protocol: http # http, tls, tcp
    port: 80 # number only
    ingress:
      enabled: true
      tls: "" # "terminated" or "" (not applicable) or "passthrough" (to be implemented)
      port: 80
      path: "/app/" # prefix regex match

# Can only do one of the httpGet and exec handler methods for livenessProbe
livenessProbe:
  # httpGet:
  #   path: /
  #   port: http
  exec:
    command:
      - sh
      - -c
      - |
        #!/usr/bin/env sh
        test -f /etc/
  failureThreshold: 5
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5

# Can only do one of the httpGet and exec handler methods for readinessProbe
readinessProbe:
  # # Handler 1
  # httpGet:
  #   path: /
  #   port: 80
  # Handler 2
  exec:
    command:
      - sh
      - -c
      - |
        #!/usr/bin/env sh
        test -f /etc/
  # Common fields
  failureThreshold: 5
  initialDelaySeconds: 10
  successThreshold: 3
  periodSeconds: 10
  timeoutSeconds: 5

########################################################################################

nameOverride: ""

keystore: []

podAnnotations:
  {}
  # iam.amazonaws.com/role: myapp-cluster

# additionals labels
labels: {}

# Allows you to load environment variables from kubernetes secret or config map
envFrom: []
# - secretRef:
#     name: env-secret
# - configMapRef:
#     name: config-map

# A list of secrets and their paths to mount inside the pod
# This is useful for mounting certificates for security and for mounting
# the X-Pack license
secretMounts: []
#  - name: myapp-certificates
#    secretName: myapp-certificates
#    path: /usr/share/myapp/config/certs
#    defaultMode: 0755

sidecarResources:
  {}
  # limits:
  #   cpu: "25m"
  #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

# networkHost: "0.0.0.0"

# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 25%
updateStrategy: RollingUpdate
# How long to wait for myapp to stop gracefully
terminationGracePeriod: 120

lifecycle:
  {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

rbac:
  create: false
  serviceAccountAnnotations: {}
  serviceAccountName: ""

podSecurityPolicy:
  create: false
  name: ""
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: "kubernetes.io/hostname"

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort"
antiAffinity: "soft"

# This is the node affinity settings as defined in
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
nodeAffinity: {}

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

podSecurityContext:
  {}
  # fsGroup: 1000
  # runAsUser: 1000

securityContext:
  {}
  # capabilities:
  #   drop:
  #     - ALL
  # # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""

nodeSelector: {}
tolerations: []

initContainer:
  enabled: false
  # command: ["echo", "I am an initContainer"]
  # image: nginx

initResources:
  {}
  # limits:
  #   cpu: "25m"
  #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

extraInitContainers:
  []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

extraVolumes:
  []
  # - name: extras
  #   emptyDir: {}

extraVolumeMounts:
  []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

extraContainers:
  []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

# Allows you to add any config files in /usr/share/myapp/config/
# as a ConfigMap
extraConfig: {}

# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs: []
#  - name: MY_ENVIRONMENT_VAR
#    value: the_value_goes_here
